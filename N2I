from pathlib import Path
import torch
from torch.utils.data import DataLoader
import torch.nn as nn
import torchvision.transforms.functional as TF
from torchvision.transforms import RandomCrop
from dataclasses import dataclass, field
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
from noise2inverse.datasets import (
    TiffDataset,
    Noise2InverseDataset,
)




# Skipping tiffs for intial hyperparameter tuning (for example skip first 100 and last 100 slices)
@dataclass







# PairedRandomCrop applies the same random crop to both input and target tensors.
@dataclass
class PairedRandomCrop:
    size: int

    @staticmethod
    def _ensure_chw(x: torch.Tensor) -> torch.Tensor:
        if x.dim() == 2:          # H, W
            return x.unsqueeze(0) # 1, H, W
        if x.dim() == 3:          # C, H, W
            return x
        raise ValueError(f"Expected 2D or 3D tensor, got shape {tuple(x.shape)}")

    def __call__(self, inp: torch.Tensor, tgt: torch.Tensor):
        inp = self._ensure_chw(inp)
        tgt = self._ensure_chw(tgt)

        i, j, h, w = RandomCrop.get_params(inp, output_size=(self.size, self.size))
        return TF.crop(inp, i, j, h, w), TF.crop(tgt, i, j, h, w)



# PatchDataset 
@dataclass
class PatchDataset(torch.utils.data.Dataset):
    base_ds: object
    patch_size: int = 256

    def __post_init__(self):
        self.crop = PairedRandomCrop(self.patch_size)

    def __len__(self):
        return len(self.base_ds)

    def __getitem__(self, idx):
        inp, tgt = self.base_ds[idx]
        return self.crop(inp, tgt)


# Class to track training and validation losses, save curves and image grids, and summarize results.
@dataclass
class Plots:
    output_dir: Path
    run_label:  str
    train_losses:    list = field(default_factory=list)
    val_losses:      list = field(default_factory=list)

    def update(self, train_loss, val_loss, metrics: dict):
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)

    def save_curves(self):
        epochs = range(1, len(self.train_losses) + 1)
        

        plt.tight_layout()
        fig.savefig(self.output_dir / f"{self.run_label}_curves.png", dpi=150)
        plt.close(fig)

    def save_image_grid(self, inp_np, pred_np, tgt_np, epoch, roi_size=64):
        """
        Shows input / predicted / target with ROI overlays
        and a difference map (pred - target).
        """
        flat_rc, edge_rc, rs = select_rois(pred_np, roi_size)

        fig, axes = plt.subplots(1, 4, figsize=(20, 5))
        fig.suptitle(f"{self.run_label}  |  epoch {epoch}", fontsize=10)
        vmin, vmax = np.percentile(tgt_np, [1, 99])

        for ax, img, title in zip(
            axes[:3],
            [inp_np, pred_np, tgt_np],
            ["Input (split recon)", "Predicted", "Target (full recon)"],
        ):
            ax.imshow(img, cmap="gray", vmin=vmin, vmax=vmax)
            ax.set_title(title, fontsize=9)
            ax.axis("off")
            # overlay ROIs on predicted panel only
            if title == "Predicted":
                for rc, color, label in [
                    (flat_rc, "cyan",   "noise ROI"),
                    (edge_rc, "yellow", "edge ROI"),
                ]:
                    rect = plt.Rectangle(
                        (rc[1], rc[0]), rs, rs,
                        linewidth=1.5, edgecolor=color, facecolor="none",
                        label=label
                    )
                    ax.add_patch(rect)
                ax.legend(fontsize=7, loc="lower right")

        diff = pred_np - tgt_np
        im = axes[3].imshow(diff, cmap="bwr",
                            vmin=-np.abs(diff).max(),
                            vmax= np.abs(diff).max())
        axes[3].set_title("Pred âˆ’ Target", fontsize=9)
        axes[3].axis("off")
        plt.colorbar(im, ax=axes[3], fraction=0.046, pad=0.04)

        plt.tight_layout()
        fig.savefig(self.output_dir / f"{self.run_label}_epoch{epoch:03d}_grid.png", dpi=150)
        plt.close(fig)

    def save_summary(self):
        best_epoch = int(np.argmin(self.noise_stds)) + 1
        summary = {
            "best_epoch_by_noise":     best_epoch,
            "best_noise_std":          float(np.min(self.noise_stds)),
            "noise_reduction_at_best": float(self.noise_reductions[best_epoch - 1]),
            "final_train_loss":        float(self.train_losses[-1]),
        }
        np.save(self.output_dir / f"{self.run_label}_summary.npy", summary)
        return summary


# Training/validation split class








# Directories
train_dir = Path("/mnt/scratch/users/mms10/recon")
output_dir = Path("weights")
output_dir.mkdir(exist_ok=True)


# Parameters
num_splits = 4
strategy = "X:1"
epochs = torch.tensor([100, 200])[0]
batch_size = torch.tensor([4, 8, 16, 32, 64])[2]
multi_gpu = False
device = torch.device("cuda")
network = "msd"
patch_size = torch.tensor([256, 512, 1024])[0]
learning_rate = torch.tensor([1e-4, 1e-3, 1e-2])[1]
training_validation_split = 0.80 # 80% for training, 20% for validation
slab_size = torch.tensor([1, 3, 5])[0] # number of adjacent slices to use as input (must be odd number)



# Scale pixel intensities during training such that its values roughly occupy the range [0,1].
# This improves convergence.
data_scaling = 200

datasets = [TiffDataset(train_dir / f"{j}/*.tiff") for j in range(num_splits)]
train_ds = PatchDataset(Noise2InverseDataset(*datasets, strategy=strategy), patch_size=patch_size)

train_ds.base_ds.num_slices, train_ds.base_ds.num_splits



# Dataloader and network:
dl = DataLoader(
    train_ds,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4, # 
    pin_memory=True, # 
    persistent_workers=True, #
)

# Option a) Use MSD network
if network == "msd":
    from msd_pytorch import MSDRegressionModel
    model = MSDRegressionModel(1, 1, 100, 1, parallel=multi_gpu)
    net = model.net
    net = model.net.to(device)
    optim = model.optimizer



# The dataset contains multiple input-target pairs for each slice. 
# Therefore, we divide by the number of splits to obtain the effective number of epochs.
train_epochs = max(epochs // num_splits, 1)

# training loop
for epoch in range(train_epochs):
    # Train
    for (inp, tgt) in tqdm(dl):
        inp = inp.to(device, non_blocking=True) * data_scaling
        tgt = tgt.to(device, non_blocking=True) * data_scaling

        # Do training step with masking
        output = net(inp)
        loss = nn.functional.mse_loss(output, tgt)
        optim.zero_grad()
        loss.backward()
        optim.step()

    # Save network 
    torch.save(
        {"epoch": int(epoch), "state_dict": net.state_dict(), "optimizer": optim.state_dict()}, 
        output_dir / f"weights_epoch_{epoch}.torch"
    )
    
torch.save(
    {"epoch": int(epoch), "state_dict": net.state_dict(), "optimizer": optim.state_dict()}, 
    output_dir / "weights.torch"
)
